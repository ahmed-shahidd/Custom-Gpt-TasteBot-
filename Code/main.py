# -*- coding: utf-8 -*-
"""gpt2_(3)_(2)_(5) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sYiD_F5WNRyn-iWVc--WuaGtnCDaGry4
"""

!pip install -r requirements.txt

import re
import PyPDF2
import json

from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments , pipeline

config_data = json.load(open("config.json"))
HF_TOKEN = config_data["HF_TOKEN"]

model_name = "openai-community/gpt2"

tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define custom data collator if necessary
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

import torch  # Ensure PyTorch is imported

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,  # Optional for GPT-2 (adjust as needed)
    use_auth_token=HF_TOKEN
)

text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=128  # Output max length
)

prompt = "What is machine learning?"
def get_response(prompt):
    sequences = text_generator(prompt)
    gen_text = sequences[0]["generated_text"]
    return gen_text

response = get_response(prompt)
print(response)

# Text cleaning function
def clean_text(text):
    text = re.sub(r"\n+", "\n", text)  # Replace multiple newlines
    text = re.sub(r"[^\x00-\x7F]+", " ", text)  # Remove non-ASCII characters
    text = re.sub(r"  +", " ", text)  # Remove extra spaces
    text = text.strip()
    # Additional cleaning steps can be added here
    return text

# Function to extract text from PDF files
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# Function to process PDF files into datasets
def process_pdf_dataset(pdf_paths):
    all_chunks = []
    for pdf_path in pdf_paths:
        raw_text = extract_text_from_pdf(pdf_path)
        cleaned_text = clean_text(raw_text)

        # Split into chunks for training
        def split_text_into_chunks(text, max_chunk_size=500):
            sentences = text.split(". ")
            chunks = []
            current_chunk = []
            current_size = 0

            for sentence in sentences:
                sentence_length = len(sentence.split())
                if current_size + sentence_length > max_chunk_size:
                    chunks.append(" ".join(current_chunk))
                    current_chunk = []
                    current_size = 0
                current_chunk.append(sentence)
                current_size += sentence_length

            if current_chunk:
                chunks.append(" ".join(current_chunk))
            return chunks

        all_chunks.extend(split_text_into_chunks(cleaned_text))

    return Dataset.from_dict({"text": all_chunks})

# Process CSV files
def process_csv_dataset(csv_paths):
    datasets = []
    for csv_path in csv_paths:
        ds = load_dataset("csv", data_files={"train": csv_path})
        ds = ds["train"]

        # Check the column names
        print("Column names:", ds.column_names)

        # Map the appropriate column to 'text'
        text_column = 'ingredients'  # Replace with the actual text column you want to use
        ds = ds.map(lambda x: {"text": x[text_column]}, remove_columns=[col for col in ds.column_names if col != text_column])

        datasets.append(ds)
    return concatenate_datasets(datasets)

# Process TXT files
def process_txt_dataset(txt_paths):
    datasets = []
    for txt_path in txt_paths:
        ds = load_dataset("text", data_files={"train": txt_path})
        ds = ds["train"]
        ds = ds.map(lambda x: {"text": clean_text(x["text"])})
        datasets.append(ds)
    return concatenate_datasets(datasets)

# Process JSON files
def process_json_dataset(json_paths):
    datasets = []
    for json_path in json_paths:
        ds = load_dataset("json", data_files={"train": json_path})
        ds = ds["train"]
        ds = ds.map(lambda x: {"text": clean_text(x["text"])}, remove_columns=ds.column_names)
        datasets.append(ds)
    return concatenate_datasets(datasets)

# Define file paths

#after finding datasets , we wil define paths of datasets also.

pdf_paths = ["/content/Zahra Azad's Recipe Book.pdf", "/content/dataset 10.pdf","/content/dataset 11.pdf","/content/dataset 12.pdf","/content/dataset 13.pdf","/content/dataset 14.pdf","/content/dataset 15.pdf","/content/dataset 16.pdf","/content/dataset 17.pdf","/content/dataset 18.pdf","/content/dataset 19.pdf","/content/dataset 2.pdf","/content/dataset 20.pdf","/content/dataset 21.pdf","/content/dataset 22.pdf","/content/dataset 23.pdf","/content/dataset 3.pdf","/content/dataset 4.pdf","/content/dataset 5.pdf","/content/dataset 6.pdf","/content/dataset 7.pdf","/content/dataset 8.pdf","/content/dataset 9.pdf","/content/dataset 24.pdf","/content/dataset 25.pdf","/content/dataset 26.pdf","/content/dataset 27.pdf","/content/dataset 28.pdf","/content/dataset 29.pdf","/content/dataset 30.pdf","/content/dataset 31.pdf","/content/dataset 32.pdf","/content/dataset 33.pdf","/content/dataset 34.pdf","/content/dataset 35.pdf","/content/dataset 36.pdf"]
# csv_paths = ["/content/dataset/recipes_data.csv"]
# txt_paths = ["path/to/txt1.txt", "path/to/txt2.txt"]
# json_paths = ["path/to/json1.json", "path/to/json2.json"]

# this code will remain same after defining correct paths for datasets. But make sure to train the model slowly  not rapidly.

pdf_dataset = process_pdf_dataset(pdf_paths)
# csv_dataset = process_csv_dataset(csv_paths)
# txt_dataset = process_txt_dataset(txt_paths)
# json_dataset = process_json_dataset(json_paths)

# Combine all datasets

combined_dataset = concatenate_datasets([pdf_dataset])

# Print the combined dataset
print(combined_dataset)

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define custom data collator if necessary
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=1,  # Adjust batch size to fit memory constraints
    num_train_epochs=3,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,  # Use gradient accumulation to simulate larger batch sizes
    fp16=True,  # Enable mixed precision training
    report_to="wandb",
)

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,  # Assuming you have an evaluation dataset
)

# Train the model
trainer.train()

import os

# Specify the output directory
output_dir = "./fine_tuned_gpt2"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r fine_tuned_gpt2.zip fine_tuned_gpt2

from google.colab import files

files.download("fine_tuned_gpt2.zip")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model_dir = "/content/fine_tuned_gpt2"

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# Force model to CPU if CUDA is not available or causes errors
# Check if CUDA is available and if the model can be moved to GPU
if torch.cuda.is_available():
    try:
        model = model.to("cuda")
        print("Model moved to GPU.")
    except RuntimeError as e:
        print(f"Error moving model to GPU: {e}")
        print("Using CPU as fallback.")
        model = model.to("cpu")  # Explicitly move to CPU
else:
    print("CUDA not available. Using CPU.")
    model = model.to("cpu")  # Explicitly move to CPU

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

# Ensure tokenizer configuration
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Define get_response function
def get_response(prompt):
    sequences = text_generator(prompt)
    gen_text = sequences[0]["generated_text"]
    return gen_text

r1 = "how to make sandwich?"
a1 = get_response(r1)
print(a1)

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, concatenate_datasets

# Load pre-trained model and tokenizer
model_name = "MiriFur/gpt2-recipes"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=1,
    num_train_epochs=5,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,
    fp16=True,
    report_to="wandb",
)

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./MiriFurgpt2-recipes")
tokenizer.save_pretrained("./MiriFurgpt2-recipes")

import os

# Specify the output directory
output_dir = "./fine_tuned_gpt2"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r MiriFurgpt2-recipes.zip MiriFurgpt2-recipes

from google.colab import files

files.download("MiriFurgpt2-recipes")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "./MiriFurgpt2-recipes"  # Path to your fine-tuned model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "Recipe for chicken soup?"
response = get_response(prompt)
print(response)

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, concatenate_datasets

# Load pre-trained model and tokenizer
model_name = "auhide/chef-gpt-en"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512  # Increase sequence length if necessary
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=1e-4,  # Adjusted learning rate
    per_device_train_batch_size=8,  # Increased batch size
    num_train_epochs=10,  # Increased number of epochs
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=4,  # Adjusted gradient accumulation steps
    fp16=True,  # Enabled mixed-precision training
    report_to="wandb",
)

# Ensure model parameters require gradients
for param in model.parameters():
    if not param.requires_grad:
        param.requires_grad = True

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Clear GPU cache before training
import torch
torch.cuda.empty_cache()

def free_gpu_memory():
    import gc
    import torch
    torch.cuda.empty_cache()
    gc.collect()

free_gpu_memory()  # Clear GPU cache before training

# Set environment variable for memory management
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,expandable_segments:True"

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Free GPU memory after training
free_gpu_memory()  # Clear GPU cache after training

# Save the model and tokenizer
model.save_pretrained("./auhide-chef-gpt-en")
tokenizer.save_pretrained("./auhide-chef-gpt-en")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "/content/auhide-chef-gpt-en"  # Path to your fine-tuned model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "Recipe for chicken soup?"
response = get_response(prompt)
print(response)

import os

# Specify the output directory
output_dir = "./auhide-chef-gpt-en"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r auhide-chef-gpt-en.zip auhide-chef-gpt-en

from google.colab import files

files.download("auhide-chef-gpt-en.zip")

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512  # Increase sequence length if necessary
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=1e-4,
    per_device_train_batch_size=2,  # Reduced batch size for memory efficiency
    num_train_epochs=5,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,  # Increased gradient accumulation steps
    fp16=True,  # Enabled mixed-precision training
    report_to="wandb",
)

# Ensure model parameters require gradients
for param in model.parameters():
    if not param.requires_grad:
        param.requires_grad = True

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Clear GPU cache before training
def free_gpu_memory():
    torch.cuda.empty_cache()
    gc.collect()

free_gpu_memory()  # Clear GPU cache before training

# Set environment variable for memory management
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64,expandable_segments:True,garbage_collection_threshold:0.6"

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Free GPU memory after training
free_gpu_memory()  # Clear GPU cache after training

# Save the model and tokenizer
model.save_pretrained("./flax-community/t5-recipe-generation")
tokenizer.save_pretrained("./flax-community/t5-recipe-generation")

from transformers import T5ForConditionalGeneration, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "/content/flax-community/t5-recipe-generation"  # Path to your fine-tuned model
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "biryani"
response = get_response(prompt)
print(response)

import os

# Specify the output directory
output_dir = "./flax-community/t5-recipe-generation"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r flax-community.zip flax-community

from google.colab import files

files.download("flax-community.zip")

import os
from huggingface_hub import HfApi, login

# Replace 'your-huggingface-username' with your actual Hugging Face username
HF_TOKEN = "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"
model_dirs = [
    "/content/fine_tuned_gpt2",
    "/content/MiriFurgpt2-recipes",
    "/content/auhide-chef-gpt-en",
    "/content/flax-community/t5-recipe-generation"
]

# Log in to Hugging Face
login(token=HF_TOKEN)

# Initialize the HfApi client
hf_api = HfApi(token=HF_TOKEN)

# Verify the token is valid by checking whoami
try:
    whoami_response = hf_api.whoami(token=HF_TOKEN)
    print("Logged in as:", whoami_response.username)
except Exception as e:
    print("Error logging in:", e)
    exit(1)

for model_dir in model_dirs:
    model_name = os.path.basename(model_dir)
    repo_id = f"muhammadAhmed22/{model_name}"

    # Create the repository if it does not exist
    try:
        hf_api.create_repo(repo_id=repo_id, repo_type="model", private=False, exist_ok=True)
    except ValueError as e:
        if "Repository already exists" not in str(e):
            raise e

    # Upload the files to the repository
    try:
        hf_api.upload_folder(
            repo_id=repo_id,
            folder_path=model_dir,
            use_auth_token=True
        )
        print(f"Model uploaded to {repo_id}")
    except Exception as e:
        print(f"Error uploading {model_dir}: {e}")

import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import PyPDF2
from concurrent.futures import ThreadPoolExecutor

# Check if GPU is available for better performance
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Optimize CUDA performance if available
if device.type == "cuda":
    torch.backends.cudnn.benchmark = True

# Load models and tokenizers with FP16 for speed optimization if GPU is available
model_dirs = [
    "muhammadAhmed22/fine_tuned_gpt2",
    "muhammadAhmed22/MiriFurgpt2-recipes",
    "muhammadAhmed22/auhide-chef-gpt-en"
]

models = {}
tokenizers = {}

for model_dir in model_dirs:
    model_name = model_dir.split("/")[-1]
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
            cache_dir="./cache"
        )
        model.to(device)

        # Compile the model for better performance (requires PyTorch 2.0+)
        if torch.__version__ >= "2.0":
            model = torch.compile(model)

        tokenizer = AutoTokenizer.from_pretrained(model_dir, cache_dir="./cache")

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        models[model_name] = model
        tokenizers[model_name] = tokenizer

        print(f"Loaded model and tokenizer from {model_dir}.")

        # Batch warm-up inference to reduce initial response time
        dummy_inputs = ["Hello", "What is a recipe?", "Explain cooking basics"]
        for dummy_input in dummy_inputs:
            input_ids = tokenizer.encode(dummy_input, return_tensors='pt').to(device)
            with torch.no_grad():
                model.generate(input_ids, max_new_tokens=1)

    except Exception as e:
        print(f"Failed to load model from {model_dir}: {e}")
        continue

def get_response(prompt, model_name, user_type):
    if model_name not in models:
        return "Model not loaded correctly."

    model = models[model_name]
    tokenizer = tokenizers[model_name]

    # Define different prompt templates based on user type
    user_type_templates = {
        "Professional Chef": f"As a professional chef, {prompt}\nAnswer:",
        "Home Cook": f"As a home cook, {prompt}\nAnswer:",
        "Beginner": f"Explain in simple terms: {prompt}\nAnswer:",
        "Food Enthusiast": f"As a food enthusiast, {prompt}\nAnswer:"
    }

    # Get the appropriate prompt based on user type
    prompt_template = user_type_templates.get(user_type, f"{prompt}\nAnswer:")

    encoding = tokenizer(
        prompt_template,
        return_tensors='pt',
        padding=True,
        truncation=True,
        max_length=300  # Increased length for larger inputs
    ).to(device)

    # Reduce max_new_tokens for faster response time
    max_new_tokens = 80  # Reduced to speed up response time

    with torch.no_grad():
        output = model.generate(
            input_ids=encoding['input_ids'],
            attention_mask=encoding['attention_mask'],
            max_new_tokens=max_new_tokens,
            num_beams=1,         # Using greedy decoding (faster)
            repetition_penalty=1.1,
            temperature=0.8,     # Slightly increased for diversity, but still quick
            top_p=0.9,           # Higher value for more flexibility in response
            early_stopping=True,
            pad_token_id=tokenizer.pad_token_id
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    return response.strip()

def extract_text_from_file(file):
    if file is None:
        return ""

    try:
        # Use multithreading for faster PDF text extraction
        with ThreadPoolExecutor() as executor:
            with open(file.name, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                texts = list(executor.map(lambda page: page.extract_text() or "", pdf_reader.pages))
                return " ".join(texts)
    except Exception as e:
        print(f"Error reading file: {e}")
        return ""

def process_input(prompt, model_name, file, user_type):
    if prompt and prompt.strip():
        return get_response(prompt, model_name, user_type)
    elif file:
        extracted_text = extract_text_from_file(file)
        if extracted_text:
            return get_response(extracted_text, model_name, user_type)
        else:
            return "Failed to extract text from the uploaded file."
    else:
        return "Please provide a prompt or upload a file."

# Gradio Interface with Modern Design
with gr.Blocks(css="""
body {
    background-color: #f4f4f4;
    font-family: 'Arial, sans-serif';
}
.title {
    font-size: 2.5rem;
    font-weight: bold;
    color: #ff6347;
    text-align: center;
    margin-bottom: 1rem;
}
.container {
    max-width: 900px;
    margin: auto;
    padding: 2rem;
    background-color: #ffffff;
    border-radius: 10px;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
}
.button {
    background-color: #ff6347;
    color: white;
    padding: 0.8rem 1.5rem;
    font-size: 1rem;
    border: none;
    border-radius: 5px;
    cursor: pointer;
}
.button:hover {
    background-color: #ff4500;
}
""") as demo:

    gr.Markdown("<div class='title'>üßëüèª‚Äçüç≥ TasteBot: Your AI Cooking Assistant</div>")

    user_types = ["Professional Chef", "Home Cook", "Beginner", "Food Enthusiast"]

    with gr.Tabs():
        with gr.TabItem("üîé Ask a Cooking Question"):
            with gr.Row():
                with gr.Column(scale=2):
                    prompt = gr.Textbox(label="üìù Prompt", placeholder="Enter your cooking question here...", lines=2)
                    model_name = gr.Radio(label="ü§ñ Choose Model", choices=list(models.keys()), interactive=True)
                    user_type = gr.Dropdown(label="üë§ User Type", choices=user_types, value="Home Cook")
                    file = gr.File(label="üìÑ Upload PDF (optional)", file_types=[".pdf"])
                    submit_button = gr.Button("Get Response", elem_classes="button")

                with gr.Column(scale=3):
                    response = gr.Textbox(
                        label="üçΩÔ∏è Response",
                        placeholder="Your answer will appear here...",
                        lines=10,
                        interactive=False,
                        show_copy_button=True
                    )

        submit_button.click(fn=process_input, inputs=[prompt, model_name, file, user_type], outputs=response)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", share=True, debug=True)