# -*- coding: utf-8 -*-
"""gpt2_(3)_(2)_(5) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sYiD_F5WNRyn-iWVc--WuaGtnCDaGry4
"""



import re
import PyPDF2
import json

from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments , pipeline

config_data = json.load(open("config.json"))
HF_TOKEN = config_data["HF_TOKEN"]

model_name = "openai-community/gpt2"

tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define custom data collator if necessary
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

import torch  # Ensure PyTorch is imported

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,  # Optional for GPT-2 (adjust as needed)
    use_auth_token=HF_TOKEN
)

text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=128  # Output max length
)

prompt = "What is machine learning?"
def get_response(prompt):
    sequences = text_generator(prompt)
    gen_text = sequences[0]["generated_text"]
    return gen_text

response = get_response(prompt)
print(response)

# Text cleaning function
def clean_text(text):
    text = re.sub(r"\n+", "\n", text)  # Replace multiple newlines
    text = re.sub(r"[^\x00-\x7F]+", " ", text)  # Remove non-ASCII characters
    text = re.sub(r"  +", " ", text)  # Remove extra spaces
    text = text.strip()
    # Additional cleaning steps can be added here
    return text

# Function to extract text from PDF files
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# Function to process PDF files into datasets
def process_pdf_dataset(pdf_paths):
    all_chunks = []
    for pdf_path in pdf_paths:
        raw_text = extract_text_from_pdf(pdf_path)
        cleaned_text = clean_text(raw_text)

        # Split into chunks for training
        def split_text_into_chunks(text, max_chunk_size=500):
            sentences = text.split(". ")
            chunks = []
            current_chunk = []
            current_size = 0

            for sentence in sentences:
                sentence_length = len(sentence.split())
                if current_size + sentence_length > max_chunk_size:
                    chunks.append(" ".join(current_chunk))
                    current_chunk = []
                    current_size = 0
                current_chunk.append(sentence)
                current_size += sentence_length

            if current_chunk:
                chunks.append(" ".join(current_chunk))
            return chunks

        all_chunks.extend(split_text_into_chunks(cleaned_text))

    return Dataset.from_dict({"text": all_chunks})

# Process CSV files
def process_csv_dataset(csv_paths):
    datasets = []
    for csv_path in csv_paths:
        ds = load_dataset("csv", data_files={"train": csv_path})
        ds = ds["train"]

        # Check the column names
        print("Column names:", ds.column_names)

        # Map the appropriate column to 'text'
        text_column = 'ingredients'  # Replace with the actual text column you want to use
        ds = ds.map(lambda x: {"text": x[text_column]}, remove_columns=[col for col in ds.column_names if col != text_column])

        datasets.append(ds)
    return concatenate_datasets(datasets)

# Process TXT files
def process_txt_dataset(txt_paths):
    datasets = []
    for txt_path in txt_paths:
        ds = load_dataset("text", data_files={"train": txt_path})
        ds = ds["train"]
        ds = ds.map(lambda x: {"text": clean_text(x["text"])})
        datasets.append(ds)
    return concatenate_datasets(datasets)

# Process JSON files
def process_json_dataset(json_paths):
    datasets = []
    for json_path in json_paths:
        ds = load_dataset("json", data_files={"train": json_path})
        ds = ds["train"]
        ds = ds.map(lambda x: {"text": clean_text(x["text"])}, remove_columns=ds.column_names)
        datasets.append(ds)
    return concatenate_datasets(datasets)

# Define file paths

#after finding datasets , we wil define paths of datasets also.

pdf_paths = ["/content/Zahra Azad's Recipe Book.pdf", "/content/dataset 10.pdf","/content/dataset 11.pdf","/content/dataset 12.pdf","/content/dataset 13.pdf","/content/dataset 14.pdf","/content/dataset 15.pdf","/content/dataset 16.pdf","/content/dataset 17.pdf","/content/dataset 18.pdf","/content/dataset 19.pdf","/content/dataset 2.pdf","/content/dataset 20.pdf","/content/dataset 21.pdf","/content/dataset 22.pdf","/content/dataset 23.pdf","/content/dataset 3.pdf","/content/dataset 4.pdf","/content/dataset 5.pdf","/content/dataset 6.pdf","/content/dataset 7.pdf","/content/dataset 8.pdf","/content/dataset 9.pdf","/content/dataset 24.pdf","/content/dataset 25.pdf","/content/dataset 26.pdf","/content/dataset 27.pdf","/content/dataset 28.pdf","/content/dataset 29.pdf","/content/dataset 30.pdf","/content/dataset 31.pdf","/content/dataset 32.pdf","/content/dataset 33.pdf","/content/dataset 34.pdf","/content/dataset 35.pdf","/content/dataset 36.pdf"]
# csv_paths = ["/content/dataset/recipes_data.csv"]
# txt_paths = ["path/to/txt1.txt", "path/to/txt2.txt"]
# json_paths = ["path/to/json1.json", "path/to/json2.json"]

# this code will remain same after defining correct paths for datasets. But make sure to train the model slowly  not rapidly.

pdf_dataset = process_pdf_dataset(pdf_paths)
# csv_dataset = process_csv_dataset(csv_paths)
# txt_dataset = process_txt_dataset(txt_paths)
# json_dataset = process_json_dataset(json_paths)

# Combine all datasets

combined_dataset = concatenate_datasets([pdf_dataset])

# Print the combined dataset
print(combined_dataset)

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define custom data collator if necessary
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=1,  # Adjust batch size to fit memory constraints
    num_train_epochs=3,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,  # Use gradient accumulation to simulate larger batch sizes
    fp16=True,  # Enable mixed precision training
    report_to="wandb",
)

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,  # Assuming you have an evaluation dataset
)

# Train the model
trainer.train()

import os

# Specify the output directory
output_dir = "./fine_tuned_gpt2"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r fine_tuned_gpt2.zip fine_tuned_gpt2

from google.colab import files

files.download("fine_tuned_gpt2.zip")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

model_dir = "/content/fine_tuned_gpt2"

# Load the model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)

# Force model to CPU if CUDA is not available or causes errors
# Check if CUDA is available and if the model can be moved to GPU
if torch.cuda.is_available():
    try:
        model = model.to("cuda")
        print("Model moved to GPU.")
    except RuntimeError as e:
        print(f"Error moving model to GPU: {e}")
        print("Using CPU as fallback.")
        model = model.to("cpu")  # Explicitly move to CPU
else:
    print("CUDA not available. Using CPU.")
    model = model.to("cpu")  # Explicitly move to CPU

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

# Ensure tokenizer configuration
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Define get_response function
def get_response(prompt):
    sequences = text_generator(prompt)
    gen_text = sequences[0]["generated_text"]
    return gen_text

r1 = "how to make sandwich?"
a1 = get_response(r1)
print(a1)

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, concatenate_datasets

# Load pre-trained model and tokenizer
model_name = "MiriFur/gpt2-recipes"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=2e-4,
    per_device_train_batch_size=1,
    num_train_epochs=5,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,
    fp16=True,
    report_to="wandb",
)

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./MiriFurgpt2-recipes")
tokenizer.save_pretrained("./MiriFurgpt2-recipes")

import os

# Specify the output directory
output_dir = "./fine_tuned_gpt2"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r MiriFurgpt2-recipes.zip MiriFurgpt2-recipes

from google.colab import files

files.download("MiriFurgpt2-recipes")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "./MiriFurgpt2-recipes"  # Path to your fine-tuned model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "Recipe for chicken soup?"
response = get_response(prompt)
print(response)

from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Dataset, concatenate_datasets

# Load pre-trained model and tokenizer
model_name = "auhide/chef-gpt-en"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512  # Increase sequence length if necessary
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=1e-4,  # Adjusted learning rate
    per_device_train_batch_size=8,  # Increased batch size
    num_train_epochs=10,  # Increased number of epochs
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=4,  # Adjusted gradient accumulation steps
    fp16=True,  # Enabled mixed-precision training
    report_to="wandb",
)

# Ensure model parameters require gradients
for param in model.parameters():
    if not param.requires_grad:
        param.requires_grad = True

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Clear GPU cache before training
import torch
torch.cuda.empty_cache()

def free_gpu_memory():
    import gc
    import torch
    torch.cuda.empty_cache()
    gc.collect()

free_gpu_memory()  # Clear GPU cache before training

# Set environment variable for memory management
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128,expandable_segments:True"

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Free GPU memory after training
free_gpu_memory()  # Clear GPU cache after training

# Save the model and tokenizer
model.save_pretrained("./auhide-chef-gpt-en")
tokenizer.save_pretrained("./auhide-chef-gpt-en")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "/content/auhide-chef-gpt-en"  # Path to your fine-tuned model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "Recipe for chicken soup?"
response = get_response(prompt)
print(response)

import os

# Specify the output directory
output_dir = "./auhide-chef-gpt-en"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r auhide-chef-gpt-en.zip auhide-chef-gpt-en

from google.colab import files

files.download("auhide-chef-gpt-en.zip")

# Tokenize Dataset
def tokenize_function(examples):
    tokenized_inputs = tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=512  # Increase sequence length if necessary
    )
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = combined_dataset.map(tokenize_function, batched=True, remove_columns=["text"])

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    run_name="my_training_run",
    eval_strategy="epoch",
    learning_rate=1e-4,
    per_device_train_batch_size=2,  # Reduced batch size for memory efficiency
    num_train_epochs=5,
    save_strategy="epoch",
    save_steps=500,
    save_total_limit=2,
    logging_dir="./logs",
    metric_for_best_model="loss",
    greater_is_better=False,
    load_best_model_at_end=True,
    gradient_accumulation_steps=8,  # Increased gradient accumulation steps
    fp16=True,  # Enabled mixed-precision training
    report_to="wandb",
)

# Ensure model parameters require gradients
for param in model.parameters():
    if not param.requires_grad:
        param.requires_grad = True

# Convert trainable weights to torch.float32
for param in model.parameters():
    if param.requires_grad:
        param.data = param.data.float()

# Clear GPU cache before training
def free_gpu_memory():
    torch.cuda.empty_cache()
    gc.collect()

free_gpu_memory()  # Clear GPU cache before training

# Set environment variable for memory management
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64,expandable_segments:True,garbage_collection_threshold:0.6"

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
)

# Train the model
trainer.train()

# Free GPU memory after training
free_gpu_memory()  # Clear GPU cache after training

# Save the model and tokenizer
model.save_pretrained("./flax-community/t5-recipe-generation")
tokenizer.save_pretrained("./flax-community/t5-recipe-generation")

from transformers import T5ForConditionalGeneration, AutoTokenizer, pipeline

# Load the fine-tuned model and tokenizer
model_name = "/content/flax-community/t5-recipe-generation"  # Path to your fine-tuned model
model = T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Ensure the tokenizer has a pad token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Create the text generation pipeline
text_generator = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    device=-1,  # Force CPU for text generation
    max_new_tokens=150
)

def get_response(prompt):
    sequences = text_generator(prompt)
    if isinstance(sequences, list) and sequences:
        # Assuming the first element of the list contains the generated text
        gen_text = sequences.get("generated_text", "") if isinstance(sequences, dict) else sequences
        if not gen_text:
            gen_text = sequences.get("text", "") if isinstance(sequences, dict) else sequences
    else:
        gen_text = ""
    return gen_text

prompt = "How to make a chicken sandwich?"
response = get_response(prompt)
print(response)

prompt = "biryani"
response = get_response(prompt)
print(response)

import os

# Specify the output directory
output_dir = "./flax-community/t5-recipe-generation"
os.makedirs(output_dir, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

!zip -r flax-community.zip flax-community

from google.colab import files

files.download("flax-community.zip")

import os
from huggingface_hub import HfApi, login

# Replace 'your-huggingface-username' with your actual Hugging Face username
HF_TOKEN = "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"
model_dirs = [
    "/content/fine_tuned_gpt2",
    "/content/MiriFurgpt2-recipes",
    "/content/auhide-chef-gpt-en",
    "/content/flax-community/t5-recipe-generation"
]

# Log in to Hugging Face
login(token=HF_TOKEN)

# Initialize the HfApi client
hf_api = HfApi(token=HF_TOKEN)

# Verify the token is valid by checking whoami
try:
    whoami_response = hf_api.whoami(token=HF_TOKEN)
    print("Logged in as:", whoami_response.username)
except Exception as e:
    print("Error logging in:", e)
    exit(1)

for model_dir in model_dirs:
    model_name = os.path.basename(model_dir)
    repo_id = f"muhammadAhmed22/{model_name}"

    # Create the repository if it does not exist
    try:
        hf_api.create_repo(repo_id=repo_id, repo_type="model", private=False, exist_ok=True)
    except ValueError as e:
        if "Repository already exists" not in str(e):
            raise e

    # Upload the files to the repository
    try:
        hf_api.upload_folder(
            repo_id=repo_id,
            folder_path=model_dir,
            use_auth_token=True
        )
        print(f"Model uploaded to {repo_id}")
    except Exception as e:
        print(f"Error uploading {model_dir}: {e}")

import gradio as gr
import os
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import PyPDF2
from concurrent.futures import ThreadPoolExecutor
import tempfile

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Check if GPU is available for better performance
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

if device.type == "cuda":
    torch.backends.cudnn.benchmark = True

# Hugging Face model paths (make sure these models exist and are accessible)
model_dirs = [
    "muhammadAhmed22/fine_tuned_gpt2",
    "muhammadAhmed22/MiriFurgpt2-recipes", 
    "muhammadAhmed22/auhide-chef-gpt-en"
]

models = {}
tokenizers = {}

def load_models():
    """Load models with better error handling"""
    loaded_models = []
    
    for model_dir in model_dirs:
        model_name = model_dir.split("/")[-1]
        try:
            logger.info(f"Loading model: {model_dir}")
            
            # Load tokenizer first
            tokenizer = AutoTokenizer.from_pretrained(
                model_dir, 
                cache_dir="./cache",
                trust_remote_code=True
            )
            
            # Load model with appropriate settings for Hugging Face Spaces
            model = AutoModelForCausalLM.from_pretrained(
                model_dir,
                torch_dtype=torch.float16 if device.type == "cuda" else torch.float32,
                cache_dir="./cache",
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            model.to(device)
            
            # Set pad token if not available
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Store models
            models[model_name] = model
            tokenizers[model_name] = tokenizer
            loaded_models.append(model_name)
            
            logger.info(f"Successfully loaded: {model_name}")
            
            # Warm up model with a simple generation
            dummy_input = "Hello"
            input_ids = tokenizer.encode(dummy_input, return_tensors='pt').to(device)
            with torch.no_grad():
                model.generate(input_ids, max_new_tokens=1, do_sample=False)
                
        except Exception as e:
            logger.error(f"Failed to load model from {model_dir}: {e}")
            continue
    
    return loaded_models

def get_response(prompt, model_name, user_type):
    """Generate response with better error handling"""
    if not prompt or not prompt.strip():
        return "Please provide a question or prompt."
    
    if model_name not in models:
        available_models = list(models.keys())
        if available_models:
            model_name = available_models[0]  # Use first available model
            logger.warning(f"Model not found, using: {model_name}")
        else:
            return "No models are currently loaded. Please try again later."

    try:
        model = models[model_name]
        tokenizer = tokenizers[model_name]

        # User type templates
        user_type_templates = {
            "Professional Chef": f"As a professional chef, {prompt}\nAnswer:",
            "Home Cook": f"As a home cook, {prompt}\nAnswer:",
            "Beginner": f"Explain in simple terms: {prompt}\nAnswer:",
            "Food Enthusiast": f"As a food enthusiast, {prompt}\nAnswer:"
        }

        prompt_template = user_type_templates.get(user_type, f"{prompt}\nAnswer:")

        # Tokenize input
        encoding = tokenizer(
            prompt_template,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=300
        ).to(device)

        # Generate response
        with torch.no_grad():
            output = model.generate(
                input_ids=encoding['input_ids'],
                attention_mask=encoding['attention_mask'],
                max_new_tokens=80,
                num_beams=1,
                repetition_penalty=1.1,
                temperature=0.8,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                do_sample=True
            )

        # Decode response
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        
        # Extract only the generated part (remove the prompt)
        if "Answer:" in response:
            response = response.split("Answer:")[-1].strip()
        
        return response if response else "I apologize, but I couldn't generate a proper response. Please try rephrasing your question."
        
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return f"Sorry, I encountered an error while processing your request. Please try again."

def extract_text_from_file(file):
    """Extract text from uploaded PDF with better error handling"""
    if file is None:
        return ""

    try:
        # Create a temporary file to handle the upload properly
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            # Copy uploaded file content to temp file
            with open(file.name, 'rb') as uploaded_file:
                tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name

        # Extract text from the temporary PDF file
        with open(tmp_file_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            text_parts = []
            
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    text = page.extract_text()
                    if text and text.strip():
                        text_parts.append(text.strip())
                except Exception as e:
                    logger.warning(f"Error extracting text from page {page_num}: {e}")
                    continue
            
            # Clean up temp file
            os.unlink(tmp_file_path)
            
            return " ".join(text_parts) if text_parts else ""
            
    except Exception as e:
        logger.error(f"Error reading PDF file: {e}")
        return ""

def process_input(prompt, model_name, file, user_type):
    """Process user input with better validation"""
    try:
        if prompt and prompt.strip():
            return get_response(prompt.strip(), model_name, user_type)
        elif file:
            extracted_text = extract_text_from_file(file)
            if extracted_text and extracted_text.strip():
                # Limit extracted text length to avoid token limits
                if len(extracted_text) > 1000:
                    extracted_text = extracted_text[:1000] + "..."
                return get_response(f"Please analyze this recipe: {extracted_text}", model_name, user_type)
            else:
                return "Failed to extract text from the uploaded file. Please make sure it's a valid PDF with readable text."
        else:
            return "Please provide a cooking question or upload a PDF file."
    except Exception as e:
        logger.error(f"Error in process_input: {e}")
        return "Sorry, I encountered an error while processing your request. Please try again."

# Enhanced CSS (keeping your existing CSS but making it more compatible)
enhanced_css = """
@import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700;900&family=Inter:wght@300;400;500;600;700&display=swap');

:root {
    --primary-orange: #FF6B35;
    --primary-red: #D32F2F;
    --warm-cream: #FFF8E7;
    --sage-green: #87A96B;
    --dark-brown: #3E2723;
    --light-gray: #F5F5F5;
    --gold: #FFD700;
    --shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
    --shadow-hover: 0 12px 40px rgba(0, 0, 0, 0.15);
}

* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    background: linear-gradient(135deg, var(--warm-cream) 0%, #FFF5E6 100%);
    font-family: 'Inter', sans-serif;
    color: var(--dark-brown);
    line-height: 1.6;
}

.hero-header {
    background: linear-gradient(135deg, var(--primary-orange) 0%, var(--primary-red) 100%);
    padding: 3rem 2rem;
    text-align: center;
    box-shadow: var(--shadow);
    position: relative;
    overflow: hidden;
}

.hero-title {
    font-family: 'Playfair Display', serif;
    font-size: 4rem;
    font-weight: 900;
    color: white;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
    margin-bottom: 1rem;
    position: relative;
    z-index: 1;
}

.hero-subtitle {
    font-size: 1.3rem;
    color: rgba(255,255,255,0.9);
    font-weight: 300;
    position: relative;
    z-index: 1;
    max-width: 600px;
    margin: 0 auto;
}

.modern-main-container {
    max-width: 1400px;
    margin: -3rem auto 4rem;
    padding: 0;
    background: transparent;
}

.content-hero {
    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);
    padding: 4rem 2rem;
    margin-bottom: 3rem;
    border-radius: 0 0 40px 40px;
    position: relative;
    overflow: hidden;
    box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
}

.content-hero-inner {
    position: relative;
    z-index: 2;
    text-align: center;
    max-width: 800px;
    margin: 0 auto;
}

.content-title {
    font-family: 'Playfair Display', serif;
    font-size: 3.5rem;
    font-weight: 900;
    color: white;
    margin-bottom: 1rem;
    text-shadow: 2px 2px 8px rgba(0,0,0,0.5);
}

.content-description {
    font-size: 1.4rem;
    color: rgba(255,255,255,0.9);
    font-weight: 300;
    line-height: 1.6;
}

.modern-layout-row {
    gap: 3rem;
    padding: 0 2rem;
    align-items: flex-start;
}

.modern-card {
    background: white;
    border-radius: 24px;
    padding: 0 !important;
    margin: 0 !important;
    box-shadow: 0 15px 35px rgba(0, 0, 0, 0.08);
    border: 1px solid rgba(255, 107, 53, 0.1);
    transition: all 0.3s cubic-bezier(0.23, 1, 0.32, 1);
    position: relative;
    overflow: hidden;
    height: auto;
}

.card-header {
    display: flex;
    align-items: center;
    gap: 1.5rem;
    margin: 0;
    padding: 2rem 2rem 1.5rem 2rem;
    background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%);
    border-radius: 24px 24px 0 0;
}

.card-icon {
    font-size: 2.5rem;
    width: 70px;
    height: 70px;
    background: linear-gradient(135deg, var(--primary-orange) 0%, var(--primary-red) 100%);
    border-radius: 20px;
    display: flex;
    align-items: center;
    justify-content: center;
    box-shadow: 0 8px 25px rgba(255, 107, 53, 0.3);
    flex-shrink: 0;
}

.chef-icon {
    background: linear-gradient(135deg, var(--sage-green) 0%, #9CAF88 100%);
    box-shadow: 0 8px 25px rgba(135, 169, 107, 0.3);
}

.card-title {
    font-family: 'Playfair Display', serif;
    font-size: 1.8rem;
    font-weight: 700;
    color: var(--dark-brown);
    margin: 0 0 0.5rem 0;
    line-height: 1.2;
}

.card-subtitle {
    font-size: 1rem;
    color: #666;
    margin: 0;
    line-height: 1.4;
}

/* Responsive Design */
@media (max-width: 1024px) {
    .modern-layout-row {
        flex-direction: column;
        gap: 2rem;
    }
    
    .content-title {
        font-size: 2.5rem;
    }
    
    .hero-title {
        font-size: 3rem;
    }
}

@media (max-width: 768px) {
    .content-hero {
        padding: 3rem 1.5rem;
    }
    
    .content-title {
        font-size: 2rem;
    }
    
    .hero-title {
        font-size: 2.5rem;
    }
    
    .modern-layout-row {
        padding: 0 1rem;
    }
}
"""

# Initialize models on startup
def initialize_app():
    """Initialize the application"""
    logger.info("Initializing TasteBot...")
    loaded_models = load_models()
    
    if not loaded_models:
        logger.warning("No models loaded successfully!")
        return ["No models available"]
    
    logger.info(f"Successfully loaded {len(loaded_models)} models: {loaded_models}")
    return loaded_models

# Load models at startup
available_models = initialize_app()
user_types = ["Professional Chef", "Home Cook", "Beginner", "Food Enthusiast"]

# Create Gradio interface
with gr.Blocks(css=enhanced_css, title="TasteBot - Culinary AI Assistant", theme=gr.themes.Soft()) as demo:
    
    # Hero Header
    gr.HTML("""
        <div class="hero-header">
            <h1 class="hero-title">🍳 TasteBot</h1>
            <p class="hero-subtitle">Your Personal Culinary AI Assistant - From Kitchen Novice to Master Chef</p>
        </div>
    """)
    
    # Main Content
    with gr.Row(elem_classes="modern-main-container"):
        with gr.Column():
            
            # Modern Hero Section for Main Content
            gr.HTML("""
                <div class="content-hero">
                    <div class="content-hero-inner">
                        <h1 class="content-title">🍳 Start Cooking with AI</h1>
                        <p class="content-description">Ask questions, get expert advice, and discover new culinary techniques</p>
                    </div>
                </div>
            """)
            
            # Main Interface
            with gr.Row(elem_classes="modern-layout-row"):
                # Left Column - Input
                with gr.Column(scale=1):
                    with gr.Column(elem_classes="modern-card"):
                        gr.HTML("""
                            <div class="card-header">
                                <div class="card-icon">💭</div>
                                <div class="card-title-section">
                                    <h3 class="card-title">Your Question</h3>
                                    <p class="card-subtitle">What cooking challenge can I help you solve?</p>
                                </div>
                            </div>
                        """)
                        
                        prompt = gr.Textbox(
                            label="Ask your cooking question",
                            placeholder="Example: How do I make perfect scrambled eggs?",
                            lines=4,
                            container=True
                        )
                    
                    user_type = gr.Dropdown(
                        label="Your Cooking Level",
                        choices=user_types,
                        value="Home Cook"
                    )
                    
                    model_name = gr.Dropdown(
                        label="Choose AI Model",
                        choices=available_models,
                        value=available_models[0] if available_models else None
                    )
                    
                    file = gr.File(
                        label="Upload Recipe PDF (Optional)",
                        file_types=[".pdf"]
                    )
                    
                    submit_button = gr.Button(
                        "✨ Get Cooking Advice",
                        variant="primary",
                        size="lg"
                    )

                # Right Column - Response
                with gr.Column(scale=1):
                    with gr.Column(elem_classes="modern-card"):
                        gr.HTML("""
                            <div class="card-header">
                                <div class="card-icon chef-icon">👨‍🍳</div>
                                <div class="card-title-section">
                                    <h3 class="card-title">Chef's Advice</h3>
                                    <p class="card-subtitle">Expert culinary guidance tailored for you</p>
                                </div>
                            </div>
                        """)
                        
                        response = gr.Textbox(
                            label="Response",
                            placeholder="Your personalized cooking advice will appear here...",
                            lines=15,
                            interactive=False,
                            show_copy_button=True,
                            container=True
                        )

    # Footer
    gr.HTML("""
        <div style="background: #3E2723; color: white; text-align: center; padding: 2rem; margin-top: 2rem;">
            <p>🍳 Powered by Advanced Culinary AI Models | Made with ❤️ for Food Lovers Everywhere</p>
        </div>
    """)

    # Event handlers
    submit_button.click(
        fn=process_input, 
        inputs=[prompt, model_name, file, user_type], 
        outputs=response
    )
    
    prompt.submit(
        fn=process_input, 
        inputs=[prompt, model_name, file, user_type], 
        outputs=response
    )

# Launch the app
if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        share=False,  # Set to False for Hugging Face Spaces
        debug=False   # Set to False for production
    )